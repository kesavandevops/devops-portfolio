# Project 01 â€“ Flask CI/CD & Canary Deployment Demo

This repository contains a complete, hands-on **DevOps portfolio project** that demonstrates an end-to-end CI/CD pipeline for a Python Flask web application using **Jenkins**, **Docker**, and **Kubernetes (AWS EKS)**. It includes Blue-Green and Canary deployment strategies and a simple traffic validation script.

---

## ğŸ§° Tech Stack

* **AWS EKS & EC2** â€“ Kubernetes cluster and Jenkins host
* **Jenkins** â€“ CI/CD automation
* **Docker** â€“ Containerization of Flask app
* **Python Flask** â€“ Sample web application
* **GitHub** â€“ Source code repository
* **Docker Hub** â€“ Container image registry
* **Kubernetes** â€“ Blue-Green & Canary deployment orchestration

---

## ğŸ“ Repo Structure

```text
01-ci-cd-pipeline/
â”œâ”€â”€ Dockerfile                   # Flask app container image
â”œâ”€â”€ Jenkinsfile                  # CI/CD pipeline
â”œâ”€â”€ canary-test.sh               # Script to test traffic distribution
â”œâ”€â”€ README.md                    # This top-level README (you are reading it)
â”œâ”€â”€ app.py                       # Flask application entry point
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ k8s-manifests/
    â”œâ”€â”€ deployment.yaml          # Blue deployment (version=blue)
    â”œâ”€â”€ deployment-green.yaml    # Green/Canary deployment (version=green)
    â””â”€â”€ service-lb.yaml          # LoadBalancer service selecting app: flask
```

---

## ğŸ“˜ Overview

This project shows how to:

* Build a Docker image for a Flask app.
* Use Jenkins to build, push to Docker Hub, and deploy to EKS.
* Deploy Blue and Green versions of the app on Kubernetes.
* Perform Canary rollouts by changing replica counts.
* Validate traffic distribution with a simple `canary-test.sh` script.

---

## ğŸ§± Prerequisites

* AWS account with EKS & EC2 access.
* Jenkins installed on an EC2 instance (or local Linux VM) with docker & kubectl configured.
* A Docker Hub account.
* `kubectl` and `aws` CLI installed and configured on the machine where you'll run commands (or on the Jenkins host).
* GitHub repo containing this project (so Jenkins can checkout the Jenkinsfile).

---

## ğŸ³ Step 1 â€” Build & Push Docker Image (local or CI)

From the `01-ci-cd-pipeline/` directory (or from pipeline):

```bash
docker build -t <dockerhub-username>/flask-cicd-app:latest .
docker push <dockerhub-username>/flask-cicd-app:latest
```

> Replace `<dockerhub-username>` with your Docker Hub username.

---

## ğŸ¤– Step 2 â€” Jenkins Pipeline (CI/CD)

Create a Jenkins Pipeline job (Pipeline script from SCM) that points to this repository and the `Jenkinsfile` path (example `01-ci-cd-pipeline/Jenkinsfile`).

**Notes:**

* Configure Jenkins credentials: `dockerhub-creds` (Docker Hub) and `aws-jenkins-creds` (AWS).
* Install Jenkins plugins: *AWS Credentials* and *Pipeline: AWS Steps* (if you use `withAWS`), and Docker Pipeline plugin.

---

## â˜¸ï¸ Step 3 â€” Apply Kubernetes Manifests (manual or CI)

Apply all manifests in the `k8s-manifests/` folder (Deployment blue, Deployment green, Service):

```bash
kubectl apply -f 01-ci-cd-pipeline/k8s-manifests/
```

This will create/update:

* `flask-deployment` (Blue) â€” labeled `app=flask, version=blue`
* `flask-deployment-green` (Green/Canary) â€” labeled `app=flask, version=green`
* `flask-service` (LoadBalancer) â€” selects `app=flask` so it routes to both versions.

---

## ğŸ” Step 4 â€” Verify deployment & service

```bash
kubectl get deployments
kubectl get pods -o wide
kubectl get svc
kubectl describe svc flask-service
```

Check that `flask-service` has an **EXTERNAL-IP** / **LoadBalancer Ingress** (DNS). Use that DNS for testing and the canary script.

---

## ğŸ§ª Step 5 â€” Smoke test the app

```bash
curl http://<LOADBALANCER_DNS>/
```

You should get responses containing either:

* `Hello from BLUE version of Flask App`
  or
* `Hello from GREEN version of Flask App`

(`app.py` uses `APP_COLOR` env variable to render this.)

---

## ğŸ”„ Step 6 â€” Canary workflow (example: 4 Blue / 1 Green)

### 6.1 Set initial counts (idempotent)

```bash
kubectl scale deployment/flask-deployment --replicas=4
kubectl scale deployment/flask-deployment-green --replicas=1

kubectl rollout status deployment/flask-deployment
kubectl rollout status deployment/flask-deployment-green
```

### 6.2 Validate endpoints & distribution

```bash
kubectl get endpoints flask-service -o wide
# Use canary-test.sh to measure distribution (see below)
```

### 6.3 Shift traffic gradually (example sequence)

Run each step, wait for rollouts to finish, then test distribution.

* Step A (initial) â€” 4 Blue / 1 Green
* Step B â€” 3 Blue / 2 Green

  ```bash
  kubectl scale deployment/flask-deployment --replicas=3
  kubectl scale deployment/flask-deployment-green --replicas=2
  kubectl rollout status deployment/flask-deployment
  kubectl rollout status deployment/flask-deployment-green
  ```
* Step C â€” 2 Blue / 3 Green

  ```bash
  kubectl scale deployment/flask-deployment --replicas=2
  kubectl scale deployment/flask-deployment-green --replicas=3
  ```
* Step D â€” 1 Blue / 4 Green
* Step E â€” 0 Blue / 5 Green (final cutover)

After each change, run the traffic test (next section) to confirm the distribution.

---

## ğŸ§ª Step 7 â€” Traffic validation script (`canary-test.sh`)

Placed `canary-test.sh` at `01-ci-cd-pipeline/canary-test.sh`. Example content:

Run:

```bash
chmod +x 01-ci-cd-pipeline/canary-test.sh
./01-ci-cd-pipeline/canary-test.sh <LOADBALANCER_DNS> 100
```

This shows the percentage split (e.g., with 4:1 you should see ~80% BLUE, ~20% GREEN).

---

## ğŸ” Step 8 â€” Rollback strategies

If the green release shows problems at any stage, rollback quickly:

* Scale green down and restore blue replicas:

  ```bash
  kubectl scale deployment/flask-deployment-green --replicas=0
  kubectl scale deployment/flask-deployment --replicas=4
  ```

* Or use rollout undo:

  ```bash
  kubectl rollout undo deployment/flask-deployment-green
  ```

---

## ğŸ§¹ Step 9 â€” Cleanup (delete cluster & resources)

**Warning:** This will remove resources and stop billing for the cluster. Ensure you do not delete shared resources used by others.

If you used `eksctl` to create the cluster, delete the nodegroups first (optional) then the cluster:

```bash
eksctl get nodegroup --cluster devops-cluster --region ap-south-1
eksctl delete nodegroup --cluster devops-cluster --name <nodegroup-name> --region ap-south-1

# delete cluster
eksctl delete cluster --name devops-cluster --region ap-south-1
```

You can also delete the cluster via `aws eks delete-cluster --name devops-cluster --region ap-south-1` and then remove EC2 instances and load balancers from the EC2 console if any remain.

---

## ğŸ’¡ Notes & Best Practices

* Use **readinessProbe** in each deployment so only healthy pods receive traffic.
* Use **livenessProbe** to restart crashed pods.
* Keep Jenkins credentials secure and use IAM roles where possible (attach IAM role to EC2 / use IRSA in production).
* For production workflows, consider introducing automated smoke tests between Canary steps to automatically progress or rollback.

